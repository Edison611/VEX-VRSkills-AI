{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "directory = '/Users/edisony611/PycharmProjects/VEX-VRSkills-AI/High Stakes/env/'\n",
    "sys.path.append(os.path.abspath(directory))\n",
    "from env import Field\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Log and Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/DQN_{int(time.time())}/\"\n",
    "logdir = f\"logs/DQN_{int(time.time())}\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "\tos.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "\tos.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Field(display=False)\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "TIMESTEPS = 10000\n",
    "iters = 0\n",
    "render_every = 10000\n",
    "render = False\n",
    "while True:\n",
    "\titers += 1\n",
    "\tmodel.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"DQN\")\n",
    "\tmodel.save(f\"{models_dir}/{TIMESTEPS*iters}\")\n",
    "\tif render:\n",
    "\t\tobs = env.reset()\n",
    "\t\tdone = False\n",
    "\t\tactions = []\n",
    "\t\twhile not done:\n",
    "\t\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\t\tactions.append(action)\n",
    "\t\t\tobs, reward, done, _ = env.step(action)\n",
    "\t\tField(display=True, actions=actions)\n",
    "\n",
    "\t# done = False\n",
    "\t# obs = env.reset()\n",
    "\t# actions = []\n",
    "\t# render = False\n",
    "\t# while not done:\n",
    "\t# \trandom_action = env.action_space.sample()\n",
    "\t# \tactions.append(random_action)\n",
    "\t# \tobs, reward, done, info = env.step(random_action)\n",
    "\t# \trender = True\n",
    "\t# if render:\n",
    "\t# \tField(display=True, actions=actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Field()\n",
    "log_num = 1724547474\n",
    "model_num = 460000\n",
    "model_path = f\"models/DQN_{log_num}/{model_num}.zip\"\n",
    "model = DQN.load(model_path, env=env)\n",
    "\n",
    "episodes = 500\n",
    "\n",
    "for episode in range(episodes):\n",
    "\tdone = False\n",
    "\tobs = env.reset()\n",
    "\tactions = []\n",
    "\trender = False\n",
    "\twhile not done:\n",
    "\t\taction, _ = model.predict(obs, deterministic=True)\n",
    "\t\tactions.append(action)\n",
    "\t\tobs, reward, done, info = env.step(action)\n",
    "\t\trender = True\n",
    "\t\t# if reward > 0.1:\n",
    "\t\t# \trender = True\n",
    "\tif render:\n",
    "\t\tField(display=True, actions=actions)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
